{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T09:23:35.031750Z",
     "start_time": "2019-05-02T09:23:35.024799Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "\n",
    "RANDOM_SEED = 12345\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import DecompensationReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(\n",
    "    data_path, 'train'), listfile=os.path.join(data_path, 'train_listfile.csv'), small_part=small_part)\n",
    "val_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(\n",
    "    data_path, 'train'), listfile=os.path.join(data_path, 'val_listfile.csv'), small_part=small_part)\n",
    "discretizer = Discretizer(timestep=arg_timestep, store_masks=True,\n",
    "                          impute_strategy='previous', start_time='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer_header = discretizer.transform(train_data_loader._data[\"X\"][0])[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = 'decomp_normalizer'\n",
    "normalizer_state = os.path.join(os.path.dirname(data_path), normalizer_state)\n",
    "normalizer.load_params(normalizer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trained_chunks = 0\n",
    "\n",
    "train_data_gen = utils.BatchGenDeepSupervision(train_data_loader, discretizer,\n",
    "                                                normalizer, batch_size, shuffle=True, return_names=True)\n",
    "val_data_gen = utils.BatchGenDeepSupervision(val_data_loader, discretizer,\n",
    "                                                normalizer, batch_size, shuffle=False, return_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueTruedemographic_data = []\n",
    "diagnosis_data = []\n",
    "idx_list = []\n",
    "t_cnt = 0\n",
    "m_cnt = []\n",
    "\n",
    "demo_path = data_path + 'demographic/'\n",
    "for cur_name in os.listdir(demo_path):\n",
    "    t_cnt+=1\n",
    "    cur_id, cur_episode = cur_name.split('_', 1)\n",
    "    cur_episode = cur_episode[:-4]\n",
    "    cur_file = demo_path + cur_name\n",
    "\n",
    "    with open(cur_file, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        if header[0] != \"Icustay\":\n",
    "            continue\n",
    "        cur_data = tsfile.readline().strip().split(',')\n",
    "        \n",
    "    if len(cur_data) == 1:\n",
    "        cur_demo = np.zeros(12)\n",
    "        cur_diag = np.zeros(128)\n",
    "    else:\n",
    "        if cur_data[3] == '':\n",
    "            cur_data[3] = 60.0\n",
    "        if cur_data[4] == '':\n",
    "            cur_data[4] = 160\n",
    "        if cur_data[5] == '':\n",
    "            cur_data[5] = 60\n",
    "\n",
    "        cur_demo = np.zeros(12)\n",
    "        cur_demo[int(cur_data[1])] = 1\n",
    "        cur_demo[5 + int(cur_data[2])] = 1\n",
    "        m_cnt.append(int(cur_data[2]))\n",
    "        cur_demo[9:] = cur_data[3:6]\n",
    "        cur_diag = np.array(cur_data[8:], dtype=np.int)\n",
    "\n",
    "    demographic_data.append(cur_demo)\n",
    "    diagnosis_data.append(cur_diag)\n",
    "    idx_list.append(cur_id+'_'+cur_episode)\n",
    "\n",
    "for each_idx in range(9,12):\n",
    "    cur_val = []\n",
    "    for i in range(len(demographic_data)):\n",
    "        cur_val.append(demographic_data[i][each_idx])\n",
    "    cur_val = np.array(cur_val)\n",
    "    _mean = np.mean(cur_val)\n",
    "    _std = np.std(cur_val)\n",
    "    _std = _std if _std > 1e-7 else 1e-7\n",
    "    for i in range(len(demographic_data)):\n",
    "        demographic_data[i][each_idx] = (demographic_data[i][each_idx] - _mean) / _std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44104699583581203"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_cnt = 0\n",
    "for val in m_cnt:\n",
    "    if val==1:\n",
    "        f_cnt+=1\n",
    "f_cnt/t_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(\"available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=9):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.nn_h = nn.Linear(channel, channel // reduction)\n",
    "        self.nn_rescale = nn.Linear(channel // reduction, channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, t = x.size()\n",
    "\n",
    "        y_pool = self.avg_pool(x).view(b, c) #B*C(*1)\n",
    "        se_h = self.nn_h(y_pool)\n",
    "        se_h = torch.relu(se_h)\n",
    "        se_h = self.nn_rescale(se_h).view(b, c, 1)\n",
    "        se_h = torch.sigmoid(se_h)\n",
    "        return x * se_h.expand_as(x), se_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stage_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, conv_dim, conv_size, output_dim, levels, dropconnect=0., dropout=0., dropres=0.3):\n",
    "        super(stage_LSTM, self).__init__()\n",
    "        \n",
    "        assert hidden_dim % levels == 0\n",
    "        self.dropout = dropout\n",
    "        self.dropconnect = dropconnect\n",
    "        self.dropres = dropres\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv_dim = conv_dim\n",
    "        self.conv_size = conv_size\n",
    "        self.output_dim = output_dim\n",
    "        self.levels = levels\n",
    "        self.chunk_size = hidden_dim // levels\n",
    "        \n",
    "        self.kernel = nn.Linear(input_dim+1, hidden_dim*4+levels*2)\n",
    "        nn.init.xavier_uniform_(self.kernel.weight)\n",
    "        nn.init.zeros_(self.kernel.bias)\n",
    "        self.recurrent_kernel = nn.Linear(hidden_dim+1, hidden_dim*4+levels*2)\n",
    "        nn.init.orthogonal_(self.recurrent_kernel.weight)\n",
    "        nn.init.zeros_(self.recurrent_kernel.bias)\n",
    "        \n",
    "        self.nn_scale = nn.Linear(hidden_dim, hidden_dim // 6)\n",
    "        self.nn_rescale = nn.Linear(hidden_dim // 6, hidden_dim)\n",
    "        self.nn_conv = nn.Conv1d(hidden_dim, conv_dim, conv_size, 1)\n",
    "        self.nn_output = nn.Linear(conv_dim, output_size)\n",
    "        \n",
    "        if self.dropconnect:\n",
    "            self.nn_dropconnect = nn.Dropout(p=dropconnect)\n",
    "            self.nn_dropconnect_r = nn.Dropout(p=dropconnect)\n",
    "        if self.dropout:\n",
    "            self.nn_dropout = nn.Dropout(p=dropout)\n",
    "            self.nn_dropres = nn.Dropout(p=dropres)\n",
    "    \n",
    "    def cumax(self, x, mode='l2r'):\n",
    "        if mode == 'l2r':\n",
    "            x = torch.softmax(x, dim=-1)\n",
    "            x = torch.cumsum(x, dim=-1)\n",
    "            return x\n",
    "        elif mode == 'r2l':\n",
    "            x = torch.flip(x, [-1])\n",
    "            x = torch.softmax(x, dim=-1)\n",
    "            x = torch.cumsum(x, dim=-1)\n",
    "            return torch.flip(x, [-1])\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def step(self, inputs, c_last, h_last):\n",
    "        x_in = inputs\n",
    "        delta_t = np.array([1.0])\n",
    "        interval = torch.ones((x_in.size(0),1),dtype=torch.float32).to(device)\n",
    "        x_out1 = self.kernel(torch.cat((x_in,interval),dim=-1))\n",
    "        x_out2 = self.recurrent_kernel(torch.cat((h_last,interval),dim=-1))\n",
    "        if self.dropconnect:\n",
    "            x_out1 = self.nn_dropconnect(x_out1)\n",
    "            x_out2 = self.nn_dropconnect_r(x_out2)\n",
    "        x_out = x_out1 + x_out2\n",
    "        f_master_gate = self.cumax(x_out[:, :self.levels], 'l2r')\n",
    "        f_master_gate = f_master_gate.unsqueeze(2)\n",
    "        i_master_gate = self.cumax(x_out[:, self.levels:self.levels*2], 'r2l')\n",
    "        i_master_gate = i_master_gate.unsqueeze(2)\n",
    "        x_out = x_out[:, self.levels*2:]\n",
    "        x_out = x_out.reshape(-1, self.levels*4, self.chunk_size)\n",
    "        f_gate = torch.sigmoid(x_out[:, :self.levels])\n",
    "        i_gate = torch.sigmoid(x_out[:, self.levels:self.levels*2])\n",
    "        o_gate = torch.sigmoid(x_out[:, self.levels*2:self.levels*3])\n",
    "        c_in = torch.tanh(x_out[:, self.levels*3:])\n",
    "        c_last = c_last.reshape(-1, self.levels, self.chunk_size)\n",
    "        overlap = f_master_gate * i_master_gate\n",
    "        c_out = overlap * (f_gate * c_last + i_gate * c_in) + (f_master_gate - overlap) * c_last + (i_master_gate - overlap) * c_in\n",
    "        h_out = o_gate * torch.tanh(c_out)\n",
    "        c_out = c_out.reshape(-1, self.hidden_dim)\n",
    "        h_out = h_out.reshape(-1, self.hidden_dim)\n",
    "        out = torch.cat([h_out, f_master_gate[..., 0], i_master_gate[..., 0]], 1)\n",
    "        return out, c_out, h_out\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        batch_size, time_step, feature_dim = input.size()\n",
    "        c_out = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        h_out = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        \n",
    "        #s*B*H\n",
    "        tmp_h = torch.zeros_like(h_out, dtype=torch.float32).view(-1).repeat(self.conv_size).view(self.conv_size, batch_size, self.hidden_dim).to(device)\n",
    "        tmp_dis = torch.zeros((self.conv_size, batch_size)).to(device)\n",
    "        h = []\n",
    "        origin_h = []\n",
    "        distance = []\n",
    "        for t in range(time_step):\n",
    "            out, c_out, h_out = self.step(input[:, t, :], c_out, h_out)\n",
    "            cur_distance = 1 - torch.mean(out[..., self.hidden_dim:self.hidden_dim+self.levels], -1)\n",
    "            cur_distance_in = torch.mean(out[..., self.hidden_dim+self.levels:], -1)\n",
    "            origin_h.append(out[..., :self.hidden_dim])\n",
    "            tmp_h = torch.cat((tmp_h[1:], out[..., :self.hidden_dim].unsqueeze(0)), 0)\n",
    "            tmp_dis = torch.cat((tmp_dis[1:], cur_distance.unsqueeze(0)), 0)\n",
    "            distance.append(cur_distance)\n",
    "            \n",
    "            local_dis = tmp_dis.permute(1, 0)\n",
    "            local_dis = torch.cumsum(local_dis, dim=1)\n",
    "            local_dis = torch.softmax(local_dis, dim=1)\n",
    "            local_h = tmp_h.permute(1, 2, 0)\n",
    "            local_h = local_h * local_dis.unsqueeze(1)\n",
    "            \n",
    "            local_theme = torch.mean(local_h, dim=-1)\n",
    "            local_theme = self.nn_scale(local_theme)\n",
    "            local_theme = torch.relu(local_theme)\n",
    "            local_theme = self.nn_rescale(local_theme)\n",
    "            local_theme = torch.sigmoid(local_theme)\n",
    "            \n",
    "            local_h = self.nn_conv(local_h).squeeze(-1)\n",
    "            local_h = local_theme * local_h\n",
    "            h.append(local_h)  \n",
    "\n",
    "        origin_h = torch.stack(origin_h).permute(1, 0, 2)\n",
    "        rnn_outputs = torch.stack(h).permute(1, 0, 2)\n",
    "        if self.dropres > 0.0:\n",
    "            origin_h = self.nn_dropres(origin_h)\n",
    "        rnn_outputs = rnn_outputs + origin_h\n",
    "        rnn_outputs = rnn_outputs.contiguous().view(-1, rnn_outputs.size(-1))\n",
    "        if self.dropout > 0.0:\n",
    "            rnn_outputs = self.nn_dropout(rnn_outputs)\n",
    "        output = self.nn_output(rnn_outputs)\n",
    "        output = output.contiguous().view(batch_size, time_step, self.output_dim)\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output, torch.stack(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 76+17\n",
    "hidden_size = 384\n",
    "conv_dim = hidden_size\n",
    "conv_size = 10\n",
    "output_size = 1\n",
    "levels = 3\n",
    "dropconnect = 0.3\n",
    "dropout = 0.3\n",
    "\n",
    "model = stage_LSTM(input_size, hidden_size, conv_dim, conv_size, output_size, levels, dropconnect, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Batch 0: Loss = 15.6404\n",
      "Chunk 0, Batch 50: Loss = 12.2789\n",
      "Chunk 0, Batch 100: Loss = 10.1171\n",
      "Chunk 0, Batch 150: Loss = 11.1317\n",
      "Chunk 0, Batch 200: Loss = 11.3013\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 14.5204\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[459434   3168]\n",
      " [  8402   2447]]\n",
      "accuracy = 0.9755623936653137\n",
      "precision class 0 = 0.9820407032966614\n",
      "precision class 1 = 0.43579697608947754\n",
      "recall class 0 = 0.9931517839431763\n",
      "recall class 1 = 0.22555074095726013\n",
      "AUC of ROC = 0.8754742790918492\n",
      "AUC of PRC = 0.26797221611047783\n",
      "min(+P, Se) = 0.32617511520737325\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Chunk 1, Batch 0: Loss = 9.2685\n",
      "Chunk 1, Batch 50: Loss = 7.8653\n",
      "Chunk 1, Batch 100: Loss = 14.6111\n",
      "Chunk 1, Batch 150: Loss = 6.4118\n",
      "Chunk 1, Batch 200: Loss = 12.4769\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 11.6407\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[489005   1250]\n",
      " [  9437   1373]]\n",
      "accuracy = 0.9786714315414429\n",
      "precision class 0 = 0.9810670018196106\n",
      "precision class 1 = 0.5234464406967163\n",
      "recall class 0 = 0.9974502921104431\n",
      "recall class 1 = 0.12701202929019928\n",
      "AUC of ROC = 0.875708831263792\n",
      "AUC of PRC = 0.2494847816063392\n",
      "min(+P, Se) = 0.31116455462029413\n",
      "\n",
      "Chunk 2, Batch 0: Loss = 11.1343\n",
      "Chunk 2, Batch 50: Loss = 15.5015\n",
      "Chunk 2, Batch 100: Loss = 7.8250\n",
      "Chunk 2, Batch 150: Loss = 12.0029\n",
      "Chunk 2, Batch 200: Loss = 6.9691\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 12.5374\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[464356   3445]\n",
      " [  8114   2730]]\n",
      "accuracy = 0.9758505821228027\n",
      "precision class 0 = 0.9828264117240906\n",
      "precision class 1 = 0.4421052634716034\n",
      "recall class 0 = 0.9926357865333557\n",
      "recall class 1 = 0.251752108335495\n",
      "AUC of ROC = 0.8840597616837788\n",
      "AUC of PRC = 0.28321406093494367\n",
      "min(+P, Se) = 0.3466433050534858\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Chunk 3, Batch 0: Loss = 6.0199\n",
      "Chunk 3, Batch 50: Loss = 8.1660\n",
      "Chunk 3, Batch 100: Loss = 6.9284\n",
      "Chunk 3, Batch 150: Loss = 8.7622\n",
      "Chunk 3, Batch 200: Loss = 14.9256\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 12.2410\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[461364   2835]\n",
      " [  7882   2553]]\n",
      "accuracy = 0.9774205088615417\n",
      "precision class 0 = 0.9832028150558472\n",
      "precision class 1 = 0.4738307297229767\n",
      "recall class 0 = 0.9938927292823792\n",
      "recall class 1 = 0.24465739727020264\n",
      "AUC of ROC = 0.8885526306541573\n",
      "AUC of PRC = 0.29699566337133976\n",
      "min(+P, Se) = 0.35218016291327264\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Chunk 4, Batch 0: Loss = 10.7398\n",
      "Chunk 4, Batch 50: Loss = 10.6287\n",
      "Chunk 4, Batch 100: Loss = 16.1317\n",
      "Chunk 4, Batch 150: Loss = 9.2339\n",
      "Chunk 4, Batch 200: Loss = 18.7224\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 12.4488\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[474643   2425]\n",
      " [  8455   2269]]\n",
      "accuracy = 0.9776954054832458\n",
      "precision class 0 = 0.9824983477592468\n",
      "precision class 1 = 0.48338302969932556\n",
      "recall class 0 = 0.9949168562889099\n",
      "recall class 1 = 0.2115814983844757\n",
      "AUC of ROC = 0.8914294071782691\n",
      "AUC of PRC = 0.28918642882260015\n",
      "min(+P, Se) = 0.34032634032634035\n",
      "\n",
      "Chunk 5, Batch 0: Loss = 11.3242\n",
      "Chunk 5, Batch 50: Loss = 17.3956\n",
      "Chunk 5, Batch 100: Loss = 8.2140\n",
      "Chunk 5, Batch 150: Loss = 10.1302\n",
      "Chunk 5, Batch 200: Loss = 8.1448\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 12.5445\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[486784   7041]\n",
      " [  7448   3786]]\n",
      "accuracy = 0.9713122844696045\n",
      "precision class 0 = 0.984930157661438\n",
      "precision class 1 = 0.3496813476085663\n",
      "recall class 0 = 0.985741913318634\n",
      "recall class 1 = 0.3370126485824585\n",
      "AUC of ROC = 0.8912032293862133\n",
      "AUC of PRC = 0.29429336317822213\n",
      "min(+P, Se) = 0.3425901201602136\n",
      "\n",
      "Chunk 6, Batch 0: Loss = 9.4059\n",
      "Chunk 6, Batch 50: Loss = 11.4060\n",
      "Chunk 6, Batch 100: Loss = 7.8352\n",
      "Chunk 6, Batch 150: Loss = 4.4810\n",
      "Chunk 6, Batch 200: Loss = 12.7772\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 12.3956\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[446118   1535]\n",
      " [  8212   2038]]\n",
      "accuracy = 0.9787138104438782\n",
      "precision class 0 = 0.9819250106811523\n",
      "precision class 1 = 0.5703890323638916\n",
      "recall class 0 = 0.9965710043907166\n",
      "recall class 1 = 0.1988292634487152\n",
      "AUC of ROC = 0.8948624798835639\n",
      "AUC of PRC = 0.3199416557331912\n",
      "min(+P, Se) = 0.3669268292682927\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Chunk 7, Batch 0: Loss = 12.2696\n",
      "Chunk 7, Batch 50: Loss = 4.6781\n",
      "Chunk 7, Batch 100: Loss = 12.5893\n",
      "Chunk 7, Batch 150: Loss = 9.0346\n",
      "Chunk 7, Batch 200: Loss = 11.0558\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 11.7435\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[466353   6425]\n",
      " [  7191   3551]]\n",
      "accuracy = 0.9718398451805115\n",
      "precision class 0 = 0.9848145246505737\n",
      "precision class 1 = 0.35595428943634033\n",
      "recall class 0 = 0.9864101409912109\n",
      "recall class 1 = 0.33057159185409546\n",
      "AUC of ROC = 0.8947718433639182\n",
      "AUC of PRC = 0.2879097541112969\n",
      "min(+P, Se) = 0.3471420592068516\n",
      "\n",
      "Chunk 8, Batch 0: Loss = 8.3448\n",
      "Chunk 8, Batch 50: Loss = 2.3884\n",
      "Chunk 8, Batch 100: Loss = 10.9129\n",
      "Chunk 8, Batch 150: Loss = 9.7485\n",
      "Chunk 8, Batch 200: Loss = 6.7635\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 12.0574\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[483167   5372]\n",
      " [  7691   3427]]\n",
      "accuracy = 0.9738560914993286\n",
      "precision class 0 = 0.984331488609314\n",
      "precision class 1 = 0.38947609066963196\n",
      "recall class 0 = 0.9890039563179016\n",
      "recall class 1 = 0.3082388937473297\n",
      "AUC of ROC = 0.8934641897001088\n",
      "AUC of PRC = 0.29215027969273283\n",
      "min(+P, Se) = 0.3548156104547082\n",
      "\n",
      "Chunk 9, Batch 0: Loss = 5.2154\n",
      "Chunk 9, Batch 50: Loss = 20.2803\n",
      "Chunk 9, Batch 100: Loss = 4.4614\n",
      "Chunk 9, Batch 150: Loss = 12.0291\n",
      "Chunk 9, Batch 200: Loss = 7.3284\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 12.4734\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[460029   3504]\n",
      " [  7489   2970]]\n",
      "accuracy = 0.9768076539039612\n",
      "precision class 0 = 0.9839813709259033\n",
      "precision class 1 = 0.4587581157684326\n",
      "recall class 0 = 0.9924406409263611\n",
      "recall class 1 = 0.28396597504615784\n",
      "AUC of ROC = 0.8914173974360101\n",
      "AUC of PRC = 0.31700874871233137\n",
      "min(+P, Se) = 0.37537049431111963\n",
      "\n",
      "Chunk 10, Batch 0: Loss = 10.6642\n",
      "Chunk 10, Batch 50: Loss = 11.0181\n",
      "Chunk 10, Batch 100: Loss = 10.1918\n",
      "Chunk 10, Batch 150: Loss = 15.0998\n",
      "Chunk 10, Batch 200: Loss = 12.7130\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 13.8437\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[457137   4835]\n",
      " [  7139   3665]]\n",
      "accuracy = 0.9746729731559753\n",
      "precision class 0 = 0.984623372554779\n",
      "precision class 1 = 0.43117648363113403\n",
      "recall class 0 = 0.9895340204238892\n",
      "recall class 1 = 0.33922621607780457\n",
      "AUC of ROC = 0.892386603578004\n",
      "AUC of PRC = 0.3476481257569219\n",
      "min(+P, Se) = 0.38744909292854496\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Chunk 11, Batch 0: Loss = 16.0938\n",
      "Chunk 11, Batch 50: Loss = 6.0071\n",
      "Chunk 11, Batch 100: Loss = 10.1398\n",
      "Chunk 11, Batch 150: Loss = 6.6592\n",
      "Chunk 11, Batch 200: Loss = 6.6754\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 11.1876\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[468618   5499]\n",
      " [  7136   3454]]\n",
      "accuracy = 0.973932683467865\n",
      "precision class 0 = 0.9850006699562073\n",
      "precision class 1 = 0.38579246401786804\n",
      "recall class 0 = 0.9884015917778015\n",
      "recall class 1 = 0.32615676522254944\n",
      "AUC of ROC = 0.8940498962792327\n",
      "AUC of PRC = 0.30208146849432654\n",
      "min(+P, Se) = 0.3579454253611557\n",
      "\n",
      "Chunk 12, Batch 0: Loss = 7.1712\n",
      "Chunk 12, Batch 50: Loss = 8.6575\n",
      "Chunk 12, Batch 100: Loss = 6.9213\n",
      "Chunk 12, Batch 150: Loss = 4.5857\n",
      "Chunk 12, Batch 200: Loss = 10.6393\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 13.3075\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[468027   6261]\n",
      " [  6961   3826]]\n",
      "accuracy = 0.9727423787117004\n",
      "precision class 0 = 0.9853448867797852\n",
      "precision class 1 = 0.3793000876903534\n",
      "recall class 0 = 0.9867991805076599\n",
      "recall class 1 = 0.3546862006187439\n",
      "AUC of ROC = 0.888559886528744\n",
      "AUC of PRC = 0.3117783208719369\n",
      "min(+P, Se) = 0.3667191698322987\n",
      "\n",
      "Chunk 13, Batch 0: Loss = 8.7979\n",
      "Chunk 13, Batch 50: Loss = 5.8990\n",
      "Chunk 13, Batch 100: Loss = 9.3296\n",
      "Chunk 13, Batch 150: Loss = 9.1422\n",
      "Chunk 13, Batch 200: Loss = 9.5679\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 12.6574\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[468447   7984]\n",
      " [  6984   3782]]\n",
      "accuracy = 0.9692773222923279\n",
      "precision class 0 = 0.9853101968765259\n",
      "precision class 1 = 0.3214346468448639\n",
      "recall class 0 = 0.9832420349121094\n",
      "recall class 1 = 0.35129109025001526\n",
      "AUC of ROC = 0.8879433361018192\n",
      "AUC of PRC = 0.2715126659868836\n",
      "min(+P, Se) = 0.3317236255572065\n",
      "\n",
      "Chunk 14, Batch 0: Loss = 10.3069\n",
      "Chunk 14, Batch 50: Loss = 5.2696\n",
      "Chunk 14, Batch 100: Loss = 6.7703\n",
      "Chunk 14, Batch 150: Loss = 6.1877\n",
      "Chunk 14, Batch 200: Loss = 7.4073\n",
      "\n",
      "==>Predicting on validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss = 13.0378\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[461830   7473]\n",
      " [  6843   3908]]\n",
      "accuracy = 0.9701783657073975\n",
      "precision class 0 = 0.9853991866111755\n",
      "precision class 1 = 0.34337931871414185\n",
      "recall class 0 = 0.9840763807296753\n",
      "recall class 1 = 0.36350107192993164\n",
      "AUC of ROC = 0.8920960561839717\n",
      "AUC of PRC = 0.28452883628349895\n",
      "min(+P, Se) = 0.3511429102397324\n",
      "\n",
      "Chunk 15, Batch 0: Loss = 6.4553\n",
      "Chunk 15, Batch 50: Loss = 14.1998\n",
      "Chunk 15, Batch 100: Loss = 7.9179\n",
      "Chunk 15, Batch 150: Loss = 22.4189\n",
      "Chunk 15, Batch 200: Loss = 9.1670\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 15.2520\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[463485   6192]\n",
      " [  7076   3740]]\n",
      "accuracy = 0.9723867177963257\n",
      "precision class 0 = 0.9849626421928406\n",
      "precision class 1 = 0.37656059861183167\n",
      "recall class 0 = 0.9868164658546448\n",
      "recall class 1 = 0.3457840383052826\n",
      "AUC of ROC = 0.8885932128354722\n",
      "AUC of PRC = 0.29940929830573665\n",
      "min(+P, Se) = 0.3615192680898253\n",
      "\n",
      "Chunk 16, Batch 0: Loss = 9.7424\n",
      "Chunk 16, Batch 50: Loss = 4.7563\n",
      "Chunk 16, Batch 100: Loss = 7.2187\n",
      "Chunk 16, Batch 150: Loss = 5.3984\n",
      "Chunk 16, Batch 200: Loss = 11.7489\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 11.7836\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[472729   3470]\n",
      " [  7841   2741]]\n",
      "accuracy = 0.9767636656761169\n",
      "precision class 0 = 0.9836839437484741\n",
      "precision class 1 = 0.44131380319595337\n",
      "recall class 0 = 0.9927131533622742\n",
      "recall class 1 = 0.2590247690677643\n",
      "AUC of ROC = 0.8898415093913195\n",
      "AUC of PRC = 0.30590236710108437\n",
      "min(+P, Se) = 0.3518413597733711\n",
      "\n",
      "Chunk 17, Batch 0: Loss = 7.2463\n",
      "Chunk 17, Batch 50: Loss = 8.7678\n",
      "Chunk 17, Batch 100: Loss = 8.6710\n",
      "Chunk 17, Batch 150: Loss = 9.3501\n",
      "Chunk 17, Batch 200: Loss = 3.0561\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 13.3189\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[467902   5073]\n",
      " [  7386   3414]]\n",
      "accuracy = 0.9742462635040283\n",
      "precision class 0 = 0.9844599366188049\n",
      "precision class 1 = 0.40226227045059204\n",
      "recall class 0 = 0.989274263381958\n",
      "recall class 1 = 0.31611111760139465\n",
      "AUC of ROC = 0.8904754218275573\n",
      "AUC of PRC = 0.3019068300951988\n",
      "min(+P, Se) = 0.3617592592592593\n",
      "\n",
      "Chunk 18, Batch 0: Loss = 7.9969\n",
      "Chunk 18, Batch 50: Loss = 5.2326\n",
      "Chunk 18, Batch 100: Loss = 10.2722\n",
      "Chunk 18, Batch 150: Loss = 7.4007\n",
      "Chunk 18, Batch 200: Loss = 14.4524\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 14.0596\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[467372   3684]\n",
      " [  7986   2720]]\n",
      "accuracy = 0.9757764339447021\n",
      "precision class 0 = 0.9832000136375427\n",
      "precision class 1 = 0.42473453283309937\n",
      "recall class 0 = 0.992179274559021\n",
      "recall class 1 = 0.25406312942504883\n",
      "AUC of ROC = 0.8875238286354646\n",
      "AUC of PRC = 0.28172729812113295\n",
      "min(+P, Se) = 0.3395539796584865\n",
      "\n",
      "Chunk 19, Batch 0: Loss = 5.2508\n",
      "Chunk 19, Batch 50: Loss = 12.9998\n",
      "Chunk 19, Batch 100: Loss = 6.8351\n",
      "Chunk 19, Batch 150: Loss = 14.2553\n",
      "Chunk 19, Batch 200: Loss = 5.3721\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 14.4316\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[467698   5408]\n",
      " [  7474   3259]]\n",
      "accuracy = 0.9733754396438599\n",
      "precision class 0 = 0.9842709302902222\n",
      "precision class 1 = 0.3760240077972412\n",
      "recall class 0 = 0.9885691404342651\n",
      "recall class 1 = 0.30364295840263367\n",
      "AUC of ROC = 0.8783755660163493\n",
      "AUC of PRC = 0.27009218232376325\n",
      "min(+P, Se) = 0.3413975993300456\n",
      "\n",
      "Chunk 20, Batch 0: Loss = 6.1532\n",
      "Chunk 20, Batch 50: Loss = 6.5058\n",
      "Chunk 20, Batch 100: Loss = 5.3054\n",
      "Chunk 20, Batch 150: Loss = 12.0342\n",
      "Chunk 20, Batch 200: Loss = 9.3100\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 14.2809\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[484108   3882]\n",
      " [  8079   2902]]\n",
      "accuracy = 0.9760286808013916\n",
      "precision class 0 = 0.98358553647995\n",
      "precision class 1 = 0.4277712404727936\n",
      "recall class 0 = 0.9920449256896973\n",
      "recall class 1 = 0.2642746567726135\n",
      "AUC of ROC = 0.8806564556710841\n",
      "AUC of PRC = 0.27997200967711633\n",
      "min(+P, Se) = 0.3370040043684019\n",
      "\n",
      "Chunk 21, Batch 0: Loss = 7.5633\n",
      "Chunk 21, Batch 50: Loss = 7.5950\n",
      "Chunk 21, Batch 100: Loss = 7.9686\n",
      "Chunk 21, Batch 150: Loss = 9.7109\n",
      "Chunk 21, Batch 200: Loss = 5.2331\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 15.8880\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[464115   5097]\n",
      " [  7593   3058]]\n",
      "accuracy = 0.9735549688339233\n",
      "precision class 0 = 0.983903169631958\n",
      "precision class 1 = 0.3749846816062927\n",
      "recall class 0 = 0.9891371130943298\n",
      "recall class 1 = 0.2871091961860657\n",
      "AUC of ROC = 0.8774714709689\n",
      "AUC of PRC = 0.2694329082747041\n",
      "min(+P, Se) = 0.3317059431039339\n",
      "\n",
      "Chunk 22, Batch 0: Loss = 8.7472\n",
      "Chunk 22, Batch 50: Loss = 11.2855\n",
      "Chunk 22, Batch 100: Loss = 14.4304\n",
      "Chunk 22, Batch 150: Loss = 16.1841\n",
      "Chunk 22, Batch 200: Loss = 4.6410\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 15.0684\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[455434   5269]\n",
      " [  7381   3262]]\n",
      "accuracy = 0.9731619954109192\n",
      "precision class 0 = 0.9840519428253174\n",
      "precision class 1 = 0.3823701739311218\n",
      "recall class 0 = 0.9885631203651428\n",
      "recall class 1 = 0.30649253726005554\n",
      "AUC of ROC = 0.8762719686584385\n",
      "AUC of PRC = 0.28906444041344426\n",
      "min(+P, Se) = 0.3495255097247017\n",
      "\n",
      "Chunk 23, Batch 0: Loss = 5.5871\n",
      "Chunk 23, Batch 50: Loss = 7.4435\n",
      "Chunk 23, Batch 100: Loss = 7.2508\n",
      "Chunk 23, Batch 150: Loss = 4.1895\n",
      "Chunk 23, Batch 200: Loss = 7.5626\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 15.0198\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[465754   6585]\n",
      " [  7388   3298]]\n",
      "accuracy = 0.9710718989372253\n",
      "precision class 0 = 0.9843852519989014\n",
      "precision class 1 = 0.3337043523788452\n",
      "recall class 0 = 0.9860587120056152\n",
      "recall class 1 = 0.308628112077713\n",
      "AUC of ROC = 0.8694461280225567\n",
      "AUC of PRC = 0.2634039622570163\n",
      "min(+P, Se) = 0.32163578513943475\n",
      "\n",
      "Chunk 24, Batch 0: Loss = 9.9982\n",
      "Chunk 24, Batch 50: Loss = 11.4088\n",
      "Chunk 24, Batch 100: Loss = 4.0990\n",
      "Chunk 24, Batch 150: Loss = 7.5195\n",
      "Chunk 24, Batch 200: Loss = 4.8351\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid loss = 16.3223\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[468070   5179]\n",
      " [  7826   2911]]\n",
      "accuracy = 0.973129391670227\n",
      "precision class 0 = 0.983555257320404\n",
      "precision class 1 = 0.3598269522190094\n",
      "recall class 0 = 0.9890565276145935\n",
      "recall class 1 = 0.27111855149269104\n",
      "AUC of ROC = 0.865969779086407\n",
      "AUC of PRC = 0.2498931014522098\n",
      "min(+P, Se) = 0.32280897829933874\n",
      "\n",
      "Chunk 25, Batch 0: Loss = 4.4423\n",
      "Chunk 25, Batch 50: Loss = 5.0665\n",
      "Chunk 25, Batch 100: Loss = 3.2348\n",
      "Chunk 25, Batch 150: Loss = 6.9771\n",
      "Chunk 25, Batch 200: Loss = 3.2742\n",
      "\n",
      "==>Predicting on validation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1b0a7f94ce86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mvalid_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mvalid_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mmasked_valid_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalid_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d6092fbdde15>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mcur_distance_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0morigin_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mtmp_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mtmp_dis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_distance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "batch_loss = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "max_auroc = 0\n",
    "max_auprc = 0\n",
    "max_len = 400\n",
    "file_name = 'sa-crnn-se2'\n",
    "\n",
    "for each_chunk in range(epochs):\n",
    "    cur_batch_loss = []\n",
    "    #train_data_gen.steps\n",
    "    model.train()\n",
    "    for each_batch in range(train_data_gen.steps):\n",
    "        starttime = datetime.datetime.now()\n",
    "        batch_data = next(train_data_gen)\n",
    "        batch_name = batch_data['names']\n",
    "        batch_data = batch_data['data']\n",
    "        \n",
    "        batch_demo = []\n",
    "        for i in range(len(batch_name)):\n",
    "            cur_id, cur_ep, _ = batch_name[i].split('_', 2)\n",
    "            cur_idx= cur_id + '_' + cur_ep\n",
    "            cur_demo = torch.tensor(demographic_data[idx_list.index(cur_idx)], dtype=torch.float32)\n",
    "            batch_demo.append(cur_demo)\n",
    "        \n",
    "        batch_demo = torch.stack(batch_demo).to(device)\n",
    "        batch_x = torch.tensor(batch_data[0][0], dtype=torch.float32).to(device)\n",
    "        batch_mask = torch.tensor(batch_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        batch_y = torch.tensor(batch_data[1], dtype=torch.float32).to(device)\n",
    "        batch_time = torch.zeros(batch_x.size(0),17, dtype=torch.float32).to(device)\n",
    "        batch_interval = torch.zeros((batch_x.size(0),batch_x.size(1),17), dtype=torch.float32).to(device)\n",
    "        \n",
    "        for i in range(batch_x.size(1)):\n",
    "            cur_ind = batch_x[:,i,-17:]\n",
    "            batch_time+=(cur_ind == 0).float()\n",
    "            batch_interval[:, i, :] = cur_ind * batch_time\n",
    "            batch_time[cur_ind==1] = 0        \n",
    "        \n",
    "        if batch_mask.size()[1] > max_len:\n",
    "            batch_x = batch_x[:, :max_len, :]\n",
    "            batch_mask = batch_mask[:, :max_len, :]\n",
    "            batch_y = batch_y[:, :max_len, :]\n",
    "            batch_interval = batch_interval[:, :max_len, :]\n",
    "        \n",
    "        batch_x = torch.cat((batch_x, batch_interval), dim=-1)\n",
    "        optimizer.zero_grad()\n",
    "        cur_output, cur_dis = model(batch_x) #B T 1\n",
    "        masked_output = cur_output * batch_mask \n",
    "        loss = batch_y * torch.log(masked_output + 1e-7) + (1 - batch_y) * torch.log(1 - masked_output + 1e-7)\n",
    "        loss = torch.sum(loss, dim=1) / torch.sum(batch_mask, dim=1)\n",
    "        loss = torch.neg(torch.sum(loss))\n",
    "        cur_batch_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if each_batch % 50 == 0:\n",
    "            print('Chunk %d, Batch %d: Loss = %.4f'%(each_chunk, each_batch, cur_batch_loss[-1]))\n",
    "\n",
    "    batch_loss.append(cur_batch_loss)\n",
    "    train_loss.append(np.mean(np.array(cur_batch_loss)))\n",
    "    \n",
    "    print(\"\\n==>Predicting on validation\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        cur_val_loss = []\n",
    "        valid_true = []\n",
    "        valid_pred = []\n",
    "        for each_batch in range(val_data_gen.steps):\n",
    "            valid_data = next(val_data_gen)\n",
    "            valid_name = valid_data['names']\n",
    "            valid_data = valid_data['data']\n",
    "            \n",
    "            valid_demo = []\n",
    "            for i in range(len(valid_name)):\n",
    "                cur_id, cur_ep, _ = valid_name[i].split('_', 2)\n",
    "                cur_idx = cur_id + '_' + cur_ep\n",
    "                cur_demo = torch.tensor(demographic_data[idx_list.index(cur_idx)], dtype=torch.float32)\n",
    "                valid_demo.append(cur_demo)\n",
    "            \n",
    "            valid_demo = torch.stack(valid_demo).to(device)\n",
    "            valid_x = torch.tensor(valid_data[0][0], dtype=torch.float32).to(device)\n",
    "            valid_mask = torch.tensor(valid_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "            valid_y = torch.tensor(valid_data[1], dtype=torch.float32).to(device)\n",
    "            valid_time = torch.zeros(valid_x.size(0),17, dtype=torch.float32).to(device)\n",
    "            valid_interval = torch.zeros((valid_x.size(0),valid_x.size(1),17), dtype=torch.float32).to(device)\n",
    "            \n",
    "            for i in range(valid_x.size(1)):\n",
    "                cur_ind = valid_x[:,i,-17:]\n",
    "                valid_time+=(cur_ind == 0).float()\n",
    "                valid_interval[:, i, :] = cur_ind * valid_time\n",
    "                valid_time[cur_ind==1] = 0  \n",
    "            \n",
    "            if valid_mask.size()[1] > max_len:\n",
    "                valid_x = valid_x[:, :max_len, :]\n",
    "                valid_mask = valid_mask[:, :max_len, :]\n",
    "                valid_y = valid_y[:, :max_len, :]\n",
    "                valid_interval = valid_interval[:, :max_len, :]\n",
    "            \n",
    "            valid_x = torch.cat((valid_x, valid_interval), dim=-1)\n",
    "            valid_output, valid_dis = model(valid_x)\n",
    "            masked_valid_output = valid_output * valid_mask\n",
    "\n",
    "            valid_loss = valid_y * torch.log(masked_valid_output + 1e-7) + (1 - valid_y) * torch.log(1 - masked_valid_output + 1e-7)\n",
    "            valid_loss = torch.sum(valid_loss, dim=1) / torch.sum(valid_mask, dim=1)\n",
    "            valid_loss = torch.neg(torch.sum(valid_loss))\n",
    "            cur_val_loss.append(valid_loss.cpu().detach().numpy())\n",
    "\n",
    "            for m, t, p in zip(valid_mask.cpu().numpy().flatten(), valid_y.cpu().numpy().flatten(), valid_output.cpu().detach().numpy().flatten()):\n",
    "                if np.equal(m, 1):\n",
    "                    valid_true.append(t)\n",
    "                    valid_pred.append(p)\n",
    "\n",
    "        val_loss.append(np.mean(np.array(cur_val_loss)))\n",
    "        print('Valid loss = %.4f'%(val_loss[-1]))\n",
    "        print('\\n')\n",
    "        valid_pred = np.array(valid_pred)\n",
    "        valid_pred = np.stack([1 - valid_pred, valid_pred], axis=1)\n",
    "        ret = metrics.print_metrics_binary(valid_true, valid_pred)\n",
    "        val_history.append(ret)\n",
    "        print()\n",
    "\n",
    "        cur_auroc = ret['auroc']\n",
    "        if cur_auroc > max_auroc:\n",
    "            max_auroc = cur_auroc\n",
    "            state = {\n",
    "                'net': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'chunk': each_chunk\n",
    "            }\n",
    "            torch.save(state, file_name+'roc')\n",
    "            #print('\\n------------ Save best model ------------\\n')\n",
    "        \n",
    "        cur_auprc = ret['auprc']\n",
    "        if cur_auprc > max_auprc:\n",
    "            max_auprc = cur_auprc\n",
    "            state = {\n",
    "                'net': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'chunk': each_chunk\n",
    "            }\n",
    "            torch.save(state, file_name)\n",
    "            print('\\n------------ Save best model ------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'sa-crnn-se2'\n",
    "checkpoint = torch.load(file_name) \n",
    "save_chunk = checkpoint['chunk']\n",
    "print(\"last saved model is in chunk {}\".format(save_chunk))\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(data_path, 'test'),\n",
    "                                                                  listfile=os.path.join(data_path, 'test_listfile.csv'), small_part=False)\n",
    "test_data_gen = utils.BatchGenDeepSupervision(test_data_loader, discretizer,\n",
    "                                              normalizer, batch_size,\n",
    "                                              shuffle=False, return_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Done (0/49)\n",
      ".....Done (1/49)\n",
      ".....Done (2/49)\n",
      ".....Done (3/49)\n",
      ".....Done (4/49)\n",
      ".....Done (5/49)\n",
      ".....Done (6/49)\n",
      ".....Done (7/49)\n",
      ".....Done (8/49)\n",
      ".....Done (9/49)\n",
      ".....Done (10/49)\n",
      ".....Done (11/49)\n",
      ".....Done (12/49)\n",
      ".....Done (13/49)\n",
      ".....Done (14/49)\n",
      ".....Done (15/49)\n",
      ".....Done (16/49)\n",
      ".....Done (17/49)\n",
      ".....Done (18/49)\n",
      ".....Done (19/49)\n",
      ".....Done (20/49)\n",
      ".....Done (21/49)\n",
      ".....Done (22/49)\n",
      ".....Done (23/49)\n",
      ".....Done (24/49)\n",
      ".....Done (25/49)\n",
      ".....Done (26/49)\n",
      ".....Done (27/49)\n",
      ".....Done (28/49)\n",
      ".....Done (29/49)\n",
      ".....Done (30/49)\n",
      ".....Done (31/49)\n",
      ".....Done (32/49)\n",
      ".....Done (33/49)\n",
      ".....Done (34/49)\n",
      ".....Done (35/49)\n",
      ".....Done (36/49)\n",
      ".....Done (37/49)\n",
      ".....Done (38/49)\n",
      ".....Done (39/49)\n",
      ".....Done (40/49)\n",
      ".....Done (41/49)\n",
      ".....Done (42/49)\n",
      ".....Done (43/49)\n",
      ".....Done (44/49)\n",
      ".....Done (45/49)\n",
      ".....Done (46/49)\n",
      ".....Done (47/49)\n",
      ".....Done (48/49)\n",
      "Test loss = 10.6832\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[462454   4025]\n",
      " [  6100   2903]]\n",
      "accuracy = 0.9787058234214783\n",
      "precision class 0 = 0.986981213092804\n",
      "precision class 1 = 0.419024258852005\n",
      "recall class 0 = 0.9913715124130249\n",
      "recall class 1 = 0.32244807481765747\n",
      "AUC of ROC = 0.9026071470555531\n",
      "AUC of PRC = 0.3228334287934801\n",
      "min(+P, Se) = 0.3715428190603132\n"
     ]
    }
   ],
   "source": [
    "#testing the model lamda=1\n",
    "max_len = 400\n",
    "with torch.no_grad():\n",
    "    cur_test_loss = []\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    \n",
    "    for each_batch in range(test_data_gen.steps):\n",
    "        test_data = next(test_data_gen)\n",
    "        test_name = test_data['names']\n",
    "        test_data = test_data['data']\n",
    "\n",
    "        test_demo = []\n",
    "        for i in range(len(test_name)):\n",
    "            cur_id, cur_ep, _ = test_name[i].split('_', 2)\n",
    "            cur_idx = cur_id + '_' + cur_ep\n",
    "\n",
    "        test_x = torch.tensor(test_data[0][0], dtype=torch.float32).to(device)\n",
    "        test_mask = torch.tensor(test_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        test_y = torch.tensor(test_data[1], dtype=torch.float32).to(device)\n",
    "        test_time = torch.zeros(test_x.size(0),17, dtype=torch.float32).to(device)\n",
    "        test_interval = torch.zeros((test_x.size(0),test_x.size(1),17), dtype=torch.float32).to(device)\n",
    "\n",
    "        for i in range(test_x.size(1)):\n",
    "            cur_ind = test_x[:,i,-17:]\n",
    "            test_time+=(cur_ind == 0).float()\n",
    "            test_interval[:, i, :] = cur_ind * test_time\n",
    "            test_time[cur_ind==1] = 0  \n",
    "        \n",
    "        if test_mask.size()[1] > max_len:\n",
    "            test_x = test_x[:, :max_len, :]\n",
    "            test_mask = test_mask[:, :max_len, :]\n",
    "            test_y = test_y[:, :max_len, :]\n",
    "            test_interval = test_interval[:, :max_len, :]\n",
    "        \n",
    "        test_x = torch.cat((test_x, test_interval), dim=-1)\n",
    "        test_output, _ = model(test_x)\n",
    "        masked_test_output = test_output * test_mask\n",
    "\n",
    "        test_loss = test_y * torch.log(masked_test_output + 1e-7) + (1 - test_y) * torch.log(1 - masked_test_output + 1e-7)\n",
    "        test_loss = torch.sum(test_loss, dim=1) / torch.sum(test_mask, dim=1)\n",
    "        test_loss = torch.neg(torch.sum(test_loss))\n",
    "        cur_test_loss.append(test_loss.cpu().detach().numpy()) \n",
    "        \n",
    "        for m, t, p in zip(test_mask.cpu().numpy().flatten(), test_y.cpu().numpy().flatten(), test_output.cpu().detach().numpy().flatten()):\n",
    "            if np.equal(m, 1):\n",
    "                test_true.append(t)\n",
    "                test_pred.append(p)\n",
    "        print('.....Done (%d/%d)'%(each_batch, test_data_gen.steps))\n",
    "    \n",
    "    print('Test loss = %.4f'%(np.mean(np.array(cur_test_loss))))\n",
    "    print('\\n')\n",
    "    test_pred = np.array(test_pred)\n",
    "    test_pred = np.stack([1 - test_pred, test_pred], axis=1)\n",
    "    test_ret = metrics.print_metrics_binary(test_true, test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
